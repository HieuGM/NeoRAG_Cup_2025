import os
import textwrap
import streamlit as st
from embeddings import Embeddings
from vector_db import VectorDatabase
from reranker import Reranker
import time

# ---------------- Page setup ----------------
st.set_page_config(page_title="NeoRAG Chat", page_icon="üß†", layout="wide")
st.title("üß† NeoRAG Chat ‚Äî ProPTIT")
st.caption("RAG ƒë∆∞·ª£c l·∫≠p tr√¨nh t·∫°i cu·ªôc thi NeoRAG-Cup-2025 c·ªßa TeamAIProPTIT")

# Subtle CSS to make it look neat
# st.markdown(
#     """
#     <style>
#       .stChatFloatingInputContainer { bottom: 1.2rem; }
#       .block-container { padding-top: 1.5rem; }
#       .small-note { color: #6b7280; font-size: 0.88rem; }
#       .ctx { background: #0b13241a; padding: .6rem .8rem; border-radius: .5rem; }
#     </style>
#     """,
#     unsafe_allow_html=True,
# )
# ---- Chat CSS ----
st.markdown("""
<style>
.chat-row {
    display: flex;
    align-items: flex-start;
    margin: 0.4rem 0;
}
.chat-avatar {
    width: 36px;
    height: 36px;
    border-radius: 50%;
}
.chat-bubble {
    padding: 0.6rem 1rem;
    margin: 0 0.5rem;
    border-radius: 1rem;
    max-width: 75%;
    font-size: 0.95rem;
    word-wrap: break-word;
    line-height: 1.4;
}
.assistant-row {
    flex-direction: row;
}
.user-row {
    flex-direction: row-reverse;   /* ƒë·∫£o ng∆∞·ª£c avatar + bubble */
}
.user-bubble {
    background-color: #DCF8C6; /* Xanh nh·∫°t WhatsApp */
    margin-left: auto;
    margin-right: 0;
    border-bottom-right-radius: 0.3rem;
}
.assistant-bubble {
    background-color: #F1F0F0;
    margin-right: auto;
    margin-left: 0;
    border-bottom-left-radius: 0.3rem;
}
</style>
""", unsafe_allow_html=True)


# ---------------- Cache heavy resources ----------------
@st.cache_resource(show_spinner=False)
def get_vector_db(db_type: str):
    return VectorDatabase(db_type=db_type)

@st.cache_resource(show_spinner=False)
def get_embedder(provider: str, model_name: str):
    return Embeddings(model_name=model_name, type=provider)

@st.cache_resource(show_spinner=False)
def get_reranker(model_name: str, use_fp16: bool = True, normalize: bool = True):
    return Reranker(model_name=model_name, use_fp16=use_fp16, normalize=normalize)

def streaming_answer(text):
    for chunk in text.split():
        yield chunk + " "
        time.sleep(0.05)
# ---------------- Core helpers ----------------
def retrieve_and_rerank(query: str, collection: str, top_k: int, top_n: int,
                        vdb: VectorDatabase, embedder: Embeddings, reranker: Reranker):
    """Retrieve top_k docs then rerank and return top_n.
    Returns (pre_results, reranked_results).
    """
    try:
        q_emb = embedder.encode(query)
    except Exception:
        st.error("Kh√¥ng t·∫°o ƒë∆∞·ª£c embedding cho truy v·∫•n. Ki·ªÉm tra API key/model.")
        return [], []

    init_limit = max(top_k, top_n) * 2
    pre_results = vdb.query(collection, q_emb, init_limit)
    if not pre_results:
        return [], []

    passages = [d.get("information", "") for d in pre_results]
    try:
        scores, ranked_passages = reranker(query, passages)
        score_map = {p: s for p, s in zip(ranked_passages, scores)}
        reranked = []
        for p in ranked_passages:
            m = next((r for r in pre_results if r.get("information", "") == p), None)
            if m:
                item = dict(m)
                item["rerank_score"] = float(score_map.get(p, 0.0))
                reranked.append(item)
        return pre_results[:top_k], reranked[:top_n]
    except Exception:
        return pre_results[:top_k], pre_results[:top_n]


def generate_response(query: str, contexts: list, llm_choice: str, model_name: str) -> str:
    context_text = "\n\n".join([f"- {d.get('information','')}" for d in contexts])
    system = (
        """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
    B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

    Nguy√™n t·∫Øc tr·∫£ l·ªùi:
    1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
    2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
    3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
    4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
    5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
    6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

    Nhi·ªám v·ª• c·ªßa b·∫°n:
    - Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c.
    """
    )
    prompt = f"Context:\n{context_text}\n\nC√¢u h·ªèi: {query}"
    try:
        if llm_choice == "gemini":
            from google import genai
            client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
            content = client.models.generate_content(
                model=model_name,
                contents=[
                    {"role": "user", "parts": [system]},
                    {"role": "user", "parts": [prompt]},
                ],
            )
            return getattr(content, "text", "").strip()
        elif llm_choice == "openai":
            from openai import OpenAI
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            stream = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system},
                    {"role": "user", "content": prompt},
                ],
                stream=True,
                temperature=0.2,
            )
            full_answer = ""
        message_holder = st.empty()  # placeholder cho UI

        for chunk in stream:
            delta = chunk.choices[0].delta
            if delta and getattr(delta, "content", None):
                full_answer += delta.content
                message_holder.markdown(full_answer + "‚ñå")

        message_holder.markdown(full_answer)  # in k·∫øt qu·∫£ cu·ªëi
        return full_answer  # tr·∫£ v·ªÅ string s·∫°ch ƒë·ªÉ l∆∞u v√†o session
    except Exception:
        return "Kh√¥ng th·ªÉ g·ªçi LLM. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh v√† API key."

# ---------------- Sidebar config ----------------
with st.sidebar:
    st.header("C·∫•u h√¨nh")
    db_type = st.selectbox("Vector DB", options=["mongodb"], index=0)
    collection = st.text_input("Collection", value="information")

    st.divider()
    st.subheader("Embedding")
    embed_provider = st.selectbox("Provider", options=["gemini"], index=0)
    default_embed = {
        "gemini": "gemini-embedding-001"
    }[embed_provider]
    emb_model = st.text_input("Model", value=default_embed)

    st.divider()
    st.subheader("Reranker")
    rerank_model_name = st.text_input("Model", value="namdp-ptit/ViRanker")
    top_k = st.slider("Top-K retrieval", 1, 20, 5)
    top_n = st.slider("Top-N sau rerank", 1, 20, 5)

    st.divider()
    st.subheader("LLM")
    llm_choice = st.selectbox("Provider", options=["gemini", "openai"], index=0)
    default_llm = {"gemini": "gemini-1.5-flash", "openai": "gpt-4o-mini"}[llm_choice]
    llm_model = st.text_input("Model", value=default_llm)

    st.divider()
    show_sources = st.toggle("Hi·ªÉn th·ªã ngu·ªìn trong c√¢u tr·∫£ l·ªùi", value=False)

    if st.button("X√≥a l·ªãch s·ª≠ chat"):
        st.session_state.clear()
        st.success("ƒê√£ x√≥a l·ªãch s·ª≠ chat.")

# ---------------- Build resources ----------------
vdb = get_vector_db(db_type)
embedder = get_embedder(embed_provider, emb_model)
reranker = get_reranker(rerank_model_name)

# ---------------- Session state ----------------
if "messages" not in st.session_state:
    st.session_state.messages = []  # list of {role, content}
if "last_contexts" not in st.session_state:
    st.session_state.last_contexts = []

# ---------------- Render chat history (top -> bottom) ----------------
# We render full history first; new messages will be added then rerun will show them at the end
for m in st.session_state.messages:
    if m["role"] == "user":
        avatar = "https://cdn-icons-png.flaticon.com/512/1946/1946429.png"
        row_class = "user-row"
        role_class = "user-bubble"
    else:
        avatar = "https://tse1.mm.bing.net/th/id/OIP.RS529nxwk4usbXFkQvrwKQHaHa?rs=1&pid=ImgDetMain&o=7&rm=3"
        row_class = "assistant-row"
        role_class = "assistant-bubble"

    st.markdown(f"""
    <div class="chat-row {row_class}">
        <img class="chat-avatar" src="{avatar}">
        <div class="chat-bubble {role_class}">{m['content']}</div>
    </div>
    """, unsafe_allow_html=True)



# ---------------- Chat input ----------------
prompt = st.chat_input("Nh·∫≠p c√¢u h·ªèi v·ªÅ CLB L·∫≠p Tr√¨nh PTIT...")
if prompt:
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.markdown(f"""
<div class="chat-row user-row">
    <img class="chat-avatar" src="https://cdn-icons-png.flaticon.com/512/1946/1946429.png">
    <div class="chat-bubble user-bubble">{prompt}</div>
</div>
""", unsafe_allow_html=True)
    # Show user's message immediately (like ChatGPT)
    # with st.chat_message("user"):
    #     st.write_stream(streaming_answer(prompt))

    # Retrieve + rerank
    with st.spinner("üîé ƒêang truy xu·∫•t v√† rerank..."):
        pre, post = retrieve_and_rerank(
            query=prompt,
            collection=collection,
            top_k=top_k,
            top_n=top_n,
            vdb=vdb,
            embedder=embedder,
            reranker=reranker,
        )
        st.session_state.pre_results = pre
        st.session_state.reranked_results = post
        st.session_state.last_contexts = post or pre

    # Generate answer
    with st.spinner("‚úçÔ∏è ƒêang t·∫°o c√¢u tr·∫£ l·ªùi‚Ä¶"):
        ans = generate_response(prompt, st.session_state.last_contexts, llm_choice, llm_model)

    # Add assistant message
    st.session_state.messages.append({"role": "assistant", "content": ans})
    st.markdown(f"""
<div class="chat-row">
    <img class="chat-avatar" src="https://tse1.mm.bing.net/th/id/OIP.RS529nxwk4usbXFkQvrwKQHaHa?rs=1&pid=ImgDetMain&o=7&rm=3">
    <div class="chat-bubble assistant-bubble">{ans}</div>
</div>
""", unsafe_allow_html=True)

    # Force rerun so that full history renders top->bottom cleanly once
    st.rerun()

st.markdown(
    """
    <style>
    .footer {
        position: fixed;
        left: 0;
        bottom: 0;
        width: 100%;
        background-color: #0b1324;
        color: white;
        text-align: center;
        padding: 8px;
        font-size: 14px;
        opacity: 0.85;
    }
    </style>
    <div class="footer">
        üöÄ Powered by <b>HieuGM</b>
    </div>
    """,
    unsafe_allow_html=True
)

# ---------------- Optional sources note ----------------
if show_sources and st.session_state.get("last_contexts"):
    st.markdown("---")
    st.subheader("Ngu·ªìn tham kh·∫£o (t√≥m t·∫Øt)")
    for i, d in enumerate(st.session_state.last_contexts, 1):
        title = d.get("title", "(no title)")
        info = d.get("information", "")
        snippet = textwrap.shorten(info, width=280, placeholder=" ‚Ä¶")
        st.markdown(f"**{i}. {title}**\n\n<div class='ctx'>{snippet}</div>", unsafe_allow_html=True)
